{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnisX8PuMzkH"
      },
      "source": [
        "# **Cheat Sheet de Machine Learning com Scikit-Learn e Pandas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1w8ZvIHM1j-"
      },
      "source": [
        "## **Pré-processamento**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by3B2VsYIt-y"
      },
      "source": [
        "### **Divisao treino/teste (`sklearn`)**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dados fictícios\n",
        "X = features\n",
        "y = target\n",
        "\n",
        "# Dividindo os dados em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "  X, y,\n",
        "  test_size=0.2,\n",
        "  random_state=42\n",
        ")\n",
        "\n",
        "# Divisão com classes desbalanceadas na resposta\n",
        "# stratify=y  # opção que mantém proporção das classes\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gBXnloWOAB5"
      },
      "source": [
        "### **OneHoteEncoding  e LabelEncoding**\n",
        "\n",
        "#### `pd.get_dummies()`\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Dados fictícios\n",
        "df = pd.DataFrame({'pay_method': ['credit', 'debit', 'pay_online']})\n",
        "\n",
        "# Codificação (novas colunas True/False)\n",
        "df_encoded = pd.get_dummies(df, columns=['pay_method'])\n",
        "```\n",
        "\n",
        "### `OneHotEncoder`\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Dados fictícios\n",
        "\n",
        "# Codificação para Arrays:\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "X = np.array(['credit', 'debit', 'pay_online']).reshape(-1, 1)\n",
        "X_encoded = encoder.fit_transform(X)\n",
        "\n",
        "# Codificação para Dataframes:\n",
        "encoder = OneHotEncoder(sparse_output=False).set_output(transform=\"pandas\")\n",
        "data_cat = data[['credit', 'debit', 'pay_online']]\n",
        "df_encoded = encoder.fit_transform(data_cat)\n",
        "```\n",
        "\n",
        "#### `LabelEncoding`\n",
        "\n",
        " ```python\n",
        " from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#  Codificação (rótulos para cada categoria)\n",
        "encoder = LabelEncoder()\n",
        "dados['categoria_cod'] = encoder.fit_transform(dados['categoria'])\n",
        "# Visualização\n",
        "print(tips['categoria'].unique())\n",
        "print(tips['categorias_cod'].unique())\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48yOZGJWOENu"
      },
      "source": [
        "### **Scaling com `StandardScaler`**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Dados fictícios\n",
        "scaler = StandardScaler()\n",
        "X = dados[['colunas_numericas']].copy()\n",
        "\n",
        "# Normalização\n",
        "X['col_std'] = scaler.fit_transform(X[['col']])\n",
        "```\n",
        "\n",
        "### Scaling com `MinMaxScaler`\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Dados fictícios\n",
        "scaler = MinMaxScaler()\n",
        "X = dados[['colunas_numericas']].copy()\n",
        "\n",
        "# Normalização\n",
        "X['col_std'] = scaler.fit_transform(X[['col']])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_wSjzVtMsbx"
      },
      "source": [
        "## **Algoritmos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Simple Linear Regression (`sklearn`)**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Dados fictícios\n",
        "X, y = ..., ...\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predições\n",
        "y_pred = model.predict(X)\n",
        "```\n",
        "\n",
        "### **Polynomial Regression (`sklearn`)**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Dados fictícios\n",
        "X, y = ..., ...\n",
        "\n",
        "# Transformação polinomial grau 3\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "x_poly = poly.fit_transform(x)\n",
        "\n",
        "# Treinar modelo\n",
        "model = LinearRegression()\n",
        "model.fit(x_poly, y)\n",
        "\n",
        "# Predições\n",
        "y_pred = model.predict(X_poly)\n",
        "```\n",
        "\n",
        "### **Logistic Regression (`sklearn`)**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Dados fictícios\n",
        "X, y = ..., ...\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = LogisticRegression(random_state=10, class_weights=None)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predições\n",
        "y_pred = model.predict(X)\n",
        "```\n",
        "\n",
        "* Alterando a probabilidade de corte:\n",
        "\n",
        "```python\n",
        "# Alterando a probabilidade de corte:\n",
        "probabilidades = model.predict_proba(features)[:, 1] # P(Y=1)\n",
        "threshold = 0.8\n",
        "classe_pred = (probabilidades >= threshold).astype(int)\n",
        "\n",
        "# Predições:\n",
        "print('Predições com threshold =', threshold)\n",
        "print(classe_pred[0:14])\n",
        "\n",
        "print('\\nProbabilidades da classe 1')\n",
        "print(np.round(probs[0:14], 2))\n",
        "```\n",
        "\n",
        "## **KNN Imputeer**\n",
        "\n",
        "```python\n",
        "# Preenchimneto de dados ausentes\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=3)\n",
        "real_estate['coluna_preenchida'] =  imputer.fit_transform(dados[['coluna']])\n",
        "```\n",
        "\n",
        "### **KNN (K-Nearest Neighbors)**\n",
        "\n",
        "```python\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Dados fictícios\n",
        "X, y = ..., ...\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = KNeighborsClassifier(n_neighbors=5)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predições\n",
        "y_pred = model.predict(X)\n",
        "```\n",
        "\n",
        "### **Naive-Bayes**\n",
        "\n",
        "```python\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Dados fictícios\n",
        "X, y = ..., ...\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = GaussianNB()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predições\n",
        "y_pred = model.predict(X)\n",
        "```\n",
        "\n",
        "## **Decision Tree Classifier (`sklearn`)**\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Dados fictícios\n",
        "X, y = ..., ...\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = DecisionTreeClassifier(random_state=10)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predições\n",
        "y_pred = model.predict(X)\n",
        "```\n",
        "\n",
        "## **Decision Tree Regressor (`sklearn`)**\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Dados fictícios\n",
        "X, y = ..., ...\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = DecisionTreeRegressor(random_state=10, criterion='entropy')\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predições\n",
        "y_pred = model.predict(X)\n",
        "```\n",
        "\n",
        "## Random Forest\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Dados fictícios\n",
        "X, y = ..., ...\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=10)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predições\n",
        "y_pred = model.predict(X)\n",
        "```\n",
        "\n",
        "## **K-means**\n",
        "\n",
        "```\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Dados fictícios\n",
        "X = ...\n",
        "\n",
        "# Treinamento do modelo\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Predições\n",
        "labels = kmeans.predict(X)\n",
        "``` \n",
        "\n",
        "## **KModes**\n",
        "\n",
        "```python\n",
        "#  (para dados categóricos)\n",
        "# Requer instalação: pip install kmodes\n",
        "from kmodes.kmodes import KModes\n",
        "\n",
        "# Exemplo para dados categóricos em DataFrame pandas\n",
        "kmodes = KModes(n_clusters=3, init='Huang', n_init=5, verbose=1, random_state=42)\n",
        "labels_kmodes = kmodes.fit_predict(X)\n",
        "print(\"KModes labels:\", labels_kmodes)\n",
        "\n",
        "# Centroides/modes dos clusters\n",
        "print(\"KModes centroids:\")\n",
        "print(kmodes.cluster_centroids_)\n",
        "```\n",
        "\n",
        "## **DBSCAN**\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)  # Ajuste esses hiperparâmetros conforme seu dado\n",
        "dbscan.fit(X)\n",
        "labels_dbscan = dbscan.labels_  # Note: -1 indica ruído (não clusterizado)\n",
        "print(\"DBSCAN labels:\", labels_dbscan)\n",
        "```\n",
        "\n",
        "## **Principal Components Analysis (PCA)**\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Obtenção das componentes principais\n",
        "pca = PCA(n_components =3)\n",
        "pca.fit(campaings)\n",
        "\n",
        "# Variância explicada (PCn)\n",
        "print(pca.explained_variance_ratio_)\n",
        "\n",
        "# Coeficientes (PCn)\n",
        "print(pca.singular_values_)\n",
        "\n",
        "# Visualização\n",
        "fig, ax = plt.subplots()\n",
        "plt.plot(range(1, len(explained_variance) + 1), pca.explained_variance_ratio_.cumsum(), 'o--k')\n",
        "plt.title('Variância explicada por componente');\n",
        "plt.xlabel('Nº Componentes');\n",
        "plt.ylabel('Variância explicada acumulada');     \n",
        "```\n",
        "\n",
        "## **Isolation Forest**\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# modelagem\n",
        "modelo_IF = IsolationForest(contamination=0.05, random_state=50)\n",
        "\n",
        "# scores\n",
        "dados['score'] = modelo_IF.decision_function(dados[colunas_interesse])\n",
        "# classe (anomalia)\n",
        "dados['anomalia'] = modelo_IF.predict(dados[colunas_interesse])\n",
        "\n",
        "# visualização (pairplot)\n",
        "sns.pairplot(dados, vars=colunas_interesse, hue='anomalia')     \n",
        "```\n",
        "\n",
        "## **LocalOutlierFactor**\n",
        "\n",
        "```python\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "cols = [...]\n",
        "\n",
        "# especificar quantos vizinhos:\n",
        "modelo_LOF = LocalOutlierFactor(n_neighbors=4, contamination='auto')\n",
        "concreto['anomalia_LOF'] = modelo_LOF.fit_predict(dados[cols])\n",
        "```\n",
        "\n",
        "## **SVM (detecção de anomalias)**\n",
        "\n",
        "```python\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "# Treinando o One-Class SVM\n",
        "oc_svm = OneClassSVM(kernel='poly', degree=3, nu=0.025, gamma='auto')\n",
        "oc_svm.fit(dados[[coluna]])\n",
        "\n",
        "# Fazendo previsões\n",
        "predictions = oc_svm.predict(dados[[coluna]])\n",
        "dados['Anomalia_SVM'] = predictions\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRTWZJbiNIJf"
      },
      "source": [
        "## **Validação**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqbVW_ISNKlG"
      },
      "source": [
        "### **Métricas de Regressão com `sklearn`**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Dados fictícios\n",
        "y_true = ...\n",
        "y_pred = ...\n",
        "\n",
        "# Métricas\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "```\n",
        "\n",
        "### **Métricas de Classificação com `sklearn`**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# Dados fictícios\n",
        "y_true = ...\n",
        "y_pred = ...\n",
        "\n",
        "# Métricas\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='binary')\n",
        "recall = recall_score(y_true, y_pred, average='binary')\n",
        "f1 = f1_score(y_true, y_pred, average='binary')\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "report = classification_report(y_true, y_pred)\n",
        "```\n",
        "\n",
        "### **Matriz de confusão e relatório**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "y_teste = ...\n",
        "y_pred = model.predict(x_teste)\n",
        "\n",
        "# Matriz de confusão para o conjunto de teste\n",
        "print(\"Matriz de confusão para o conjunto de teste:\")\n",
        "print(confusion_matrix(y_teste, ypred_teste))\n",
        "\n",
        "# Visualizando a matriz  de confusão em gráfico\n",
        "matriz = confusion_matrix(y_teste, ypred_teste)\n",
        "plt.figure(figsize=(4,3))\n",
        "sns.heatmap(matriz,annot=True,fmt='d')\n",
        "plt.xlabel('Valores Preditos');\n",
        "plt.ylabel('Valores Reais');\n",
        "plt.title('Matriz de Confusão');\n",
        "\n",
        "# Relatório\n",
        "print(classification_report(y_teste, ypred_teste))\n",
        "```\n",
        "\n",
        "### **Curva ROC e AUC**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "y_teste = ...\n",
        "y_pred = model.predict(x_teste)\n",
        "\n",
        "# Calcular a curva ROC e a AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_teste, ypred_teste)\n",
        "roc_auc = roc_auc_score(y_teste, ypred_teste)\n",
        "\n",
        "# Plotar a curva ROC\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "plt.plot(fpr, tpr, color='maroon', lw=2, label=f'Curva ROC (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Taxa de Falsos Positivos')\n",
        "plt.ylabel('Taxa de Verdadeiros Positivos')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "```\n",
        "## **Visualizar árvore DT**\n",
        "\n",
        "```python\n",
        "import graphviz\n",
        "from sklearn import tree\n",
        "\n",
        "tree_data = tree.export_graphviz(model_DT, out_file=None)\n",
        "graph = graphviz.Source(tree_data)\n",
        "graph\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtsz6rLVNfjm"
      },
      "source": [
        "## **Exportando um modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq0_MdJ2NdkH"
      },
      "source": [
        "\n",
        "```python\n",
        "import pickle\n",
        "\n",
        "with open('rf.pkl', 'wb') as file:\n",
        "  pickle.dump(rf, file)\n",
        "```\n",
        "\n",
        "```python\n",
        "with open('rf.pkl', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "loaded_model\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
