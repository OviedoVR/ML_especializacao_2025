{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnisX8PuMzkH"
      },
      "source": [
        "# **Cheat Sheet de Machine Learning com Scikit-Learn e Pandas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1w8ZvIHM1j-"
      },
      "source": [
        "## **Pré-processamento**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by3B2VsYIt-y"
      },
      "source": [
        "### **Divisao treino/teste (`sklearn`)**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dados fictícios\n",
        "X = features\n",
        "y = target\n",
        "\n",
        "# Dividindo os dados em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "  X, y,\n",
        "  test_size=0.2,\n",
        "  random_state=42\n",
        ")\n",
        "\n",
        "# Divisão com classes desbalanceadas na resposta\n",
        "# stratify=y  # opção que mantém proporção das classes\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gBXnloWOAB5"
      },
      "source": [
        "### **OneHoteEncoding  e LabelEncoding**\n",
        "\n",
        "#### `pd.get_dummies()`\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Dados fictícios\n",
        "df = pd.DataFrame({'pay_method': ['credit', 'debit', 'pay_online']})\n",
        "\n",
        "# Codificação (novas colunas True/False)\n",
        "df_encoded = pd.get_dummies(df, columns=['pay_method'], dtype=int)\n",
        "```\n",
        "\n",
        "### `OneHotEncoder`\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Dados fictícios\n",
        "\n",
        "# Codificação para Arrays:\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "X = np.array(['credit', 'debit', 'pay_online']).reshape(-1, 1)\n",
        "X_encoded = encoder.fit_transform(X)\n",
        "```\n",
        "\n",
        "#### `LabelEncoding`\n",
        "\n",
        " ```python\n",
        " from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#  Codificação (rótulos para cada categoria)\n",
        "encoder = LabelEncoder()\n",
        "dados['categoria_cod'] = encoder.fit_transform(dados['categoria'])\n",
        "# Visualização\n",
        "print(tips['categoria'].unique())\n",
        "print(tips['categorias_cod'].unique())\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48yOZGJWOENu"
      },
      "source": [
        "### **Scaling com `StandardScaler`**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Dados fictícios\n",
        "scaler = StandardScaler()\n",
        "X = dados[['colunas_numericas']].copy()\n",
        "\n",
        "# Normalização\n",
        "X['col_std'] = scaler.fit_transform(X[['col']])\n",
        "```\n",
        "\n",
        "### Scaling com `MinMaxScaler`\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Dados fictícios\n",
        "scaler = MinMaxScaler()\n",
        "X = dados[['colunas_numericas']].copy()\n",
        "\n",
        "# Normalização\n",
        "X['col_std'] = scaler.fit_transform(X[['col']])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_wSjzVtMsbx"
      },
      "source": [
        "## **Algoritmos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Simple Linear Regression (`sklearn`)**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Dados fictícios\n",
        "X, y = ..., ...\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predições\n",
        "y_pred = model.predict(X)\n",
        "```\n",
        "\n",
        "### **Polynomial Regression (`sklearn`)**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Dados fictícios\n",
        "X, y = ..., ...\n",
        "\n",
        "# Transformação polinomial grau 3\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "x_poly = poly.fit_transform(x)\n",
        "\n",
        "# Treinar modelo\n",
        "model = LinearRegression()\n",
        "model.fit(x_poly, y)\n",
        "\n",
        "# Predições\n",
        "y_pred = model.predict(X_poly)\n",
        "```\n",
        "\n",
        "### **Logistic Regression (`sklearn`)**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Dados fictícios\n",
        "X, y = ..., ...\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = LogisticRegression(random_state=10, class_weights=None)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predições\n",
        "y_pred = model.predict(X)\n",
        "```\n",
        "\n",
        "* Alterando a probabilidade de corte:\n",
        "\n",
        "```python\n",
        "# Alterando a probabilidade de corte:\n",
        "probabilidades = model.predict_proba(features)[:, 1] # P(Y=1)\n",
        "threshold = 0.8\n",
        "classe_pred = (probabilidades >= threshold).astype(int)\n",
        "\n",
        "# Predições:\n",
        "print('Predições com threshold =', threshold)\n",
        "print(classe_pred[0:14])\n",
        "\n",
        "print('\\nProbabilidades da classe 1')\n",
        "print(np.round(probs[0:14], 2))\n",
        "```\n",
        "\n",
        "## **KNN Imputeer**\n",
        "\n",
        "```python\n",
        "# Preenchimneto de dados ausentes\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=3)\n",
        "real_estate['coluna_preenchida'] =  imputer.fit_transform(dados[['coluna']])\n",
        "```\n",
        "\n",
        "### **KNN (K-Nearest Neighbors)**\n",
        "\n",
        "```python\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Dados fictícios\n",
        "X, y = ..., ...\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = KNeighborsClassifier(n_neighbors=5)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predições\n",
        "y_pred = model.predict(X)\n",
        "```\n",
        "\n",
        "### **Naive-Bayes**\n",
        "\n",
        "```python\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Dados fictícios\n",
        "X, y = ..., ...\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = GaussianNB()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predições\n",
        "y_pred = model.predict(X)\n",
        "```\n",
        "\n",
        "## **Decision Tree Classifier (`sklearn`)**\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Dados fictícios\n",
        "X, y = ..., ...\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = DecisionTreeClassifier(random_state=10)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predições\n",
        "y_pred = model.predict(X)\n",
        "```\n",
        "\n",
        "## **Decision Tree Regressor (`sklearn`)**\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Dados fictícios\n",
        "X, y = ..., ...\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = DecisionTreeRegressor(random_state=10, criterion='entropy')\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predições\n",
        "y_pred = model.predict(X)\n",
        "```\n",
        "\n",
        "## Random Forest\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Dados fictícios\n",
        "X, y = ..., ...\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=10)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predições\n",
        "y_pred = model.predict(X)\n",
        "```\n",
        "\n",
        "## **K-means**\n",
        "\n",
        "```\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Dados fictícios\n",
        "X = ...\n",
        "\n",
        "# Treinamento do modelo\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Predições\n",
        "labels = kmeans.predict(X)\n",
        "``` \n",
        "\n",
        "## **KModes**\n",
        "\n",
        "```python\n",
        "#  (para dados categóricos)\n",
        "# Requer instalação: pip install kmodes\n",
        "from kmodes.kmodes import KModes\n",
        "\n",
        "# Exemplo para dados categóricos em DataFrame pandas\n",
        "kmodes = KModes(n_clusters=3, init='Huang', n_init=5, verbose=1, random_state=42)\n",
        "labels_kmodes = kmodes.fit_predict(X)\n",
        "print(\"KModes labels:\", labels_kmodes)\n",
        "\n",
        "# Centroides/modes dos clusters\n",
        "print(\"KModes centroids:\")\n",
        "print(kmodes.cluster_centroids_)\n",
        "```\n",
        "\n",
        "## **DBSCAN**\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)  # Ajuste esses hiperparâmetros conforme seu dado\n",
        "dbscan.fit(X)\n",
        "labels_dbscan = dbscan.labels_  # Note: -1 indica ruído (não clusterizado)\n",
        "print(\"DBSCAN labels:\", labels_dbscan)\n",
        "```\n",
        "\n",
        "## **Principal Components Analysis (PCA)**\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Obtenção das componentes principais\n",
        "pca = PCA(n_components =3)\n",
        "pca.fit(campaings)\n",
        "\n",
        "# Variância explicada (PCn)\n",
        "print(pca.explained_variance_ratio_)\n",
        "\n",
        "# Coeficientes (PCn)\n",
        "print(pca.singular_values_)\n",
        "\n",
        "# Visualização\n",
        "fig, ax = plt.subplots()\n",
        "plt.plot(range(1, len(explained_variance) + 1), pca.explained_variance_ratio_.cumsum(), 'o--k')\n",
        "plt.title('Variância explicada por componente');\n",
        "plt.xlabel('Nº Componentes');\n",
        "plt.ylabel('Variância explicada acumulada');     \n",
        "```\n",
        "\n",
        "## **Isolation Forest**\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# modelagem\n",
        "modelo_IF = IsolationForest(contamination=0.05, random_state=50)\n",
        "\n",
        "# scores\n",
        "dados['score'] = modelo_IF.decision_function(dados[colunas_interesse])\n",
        "# classe (anomalia)\n",
        "dados['anomalia'] = modelo_IF.predict(dados[colunas_interesse])\n",
        "\n",
        "# visualização (pairplot)\n",
        "sns.pairplot(dados, vars=colunas_interesse, hue='anomalia')     \n",
        "```\n",
        "\n",
        "## **LocalOutlierFactor**\n",
        "\n",
        "```python\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "cols = [...]\n",
        "\n",
        "# especificar quantos vizinhos:\n",
        "modelo_LOF = LocalOutlierFactor(n_neighbors=4, contamination='auto')\n",
        "concreto['anomalia_LOF'] = modelo_LOF.fit_predict(dados[cols])\n",
        "```\n",
        "\n",
        "## **SVM (detecção de anomalias)**\n",
        "\n",
        "```python\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "# Treinando o One-Class SVM\n",
        "oc_svm = OneClassSVM(kernel='poly', degree=3, nu=0.025, gamma='auto')\n",
        "oc_svm.fit(dados[[coluna]])\n",
        "\n",
        "# Fazendo previsões\n",
        "predictions = oc_svm.predict(dados[[coluna]])\n",
        "dados['Anomalia_SVM'] = predictions\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRTWZJbiNIJf"
      },
      "source": [
        "## **Validação**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqbVW_ISNKlG"
      },
      "source": [
        "### **Métricas de Regressão com `sklearn`**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Dados fictícios\n",
        "y_true = ...\n",
        "y_pred = ...\n",
        "\n",
        "# Métricas\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "```\n",
        "\n",
        "### **Métricas de Classificação com `sklearn`**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# Dados fictícios\n",
        "y_true = ...\n",
        "y_pred = ...\n",
        "\n",
        "# Métricas\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='binary')\n",
        "recall = recall_score(y_true, y_pred, average='binary')\n",
        "f1 = f1_score(y_true, y_pred, average='binary')\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "report = classification_report(y_true, y_pred)\n",
        "```\n",
        "\n",
        "### **Matriz de confusão e relatório**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "y_teste = ...\n",
        "y_pred = model.predict(x_teste)\n",
        "\n",
        "# Matriz de confusão para o conjunto de teste\n",
        "print(\"Matriz de confusão para o conjunto de teste:\")\n",
        "print(confusion_matrix(y_teste, ypred_teste))\n",
        "\n",
        "# Visualizando a matriz  de confusão em gráfico\n",
        "matriz = confusion_matrix(y_teste, ypred_teste)\n",
        "plt.figure(figsize=(4,3))\n",
        "sns.heatmap(matriz,annot=True,fmt='d')\n",
        "plt.xlabel('Valores Preditos');\n",
        "plt.ylabel('Valores Reais');\n",
        "plt.title('Matriz de Confusão');\n",
        "\n",
        "# Relatório\n",
        "print(classification_report(y_teste, ypred_teste))\n",
        "```\n",
        "\n",
        "### **Curva ROC e AUC**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "y_teste = ...\n",
        "y_pred = model.predict(x_teste)\n",
        "\n",
        "# Calcular a curva ROC e a AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_teste, ypred_teste)\n",
        "roc_auc = roc_auc_score(y_teste, ypred_teste)\n",
        "\n",
        "# Plotar a curva ROC\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "plt.plot(fpr, tpr, color='maroon', lw=2, label=f'Curva ROC (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Taxa de Falsos Positivos')\n",
        "plt.ylabel('Taxa de Verdadeiros Positivos')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "```\n",
        "## **Visualizar árvore DT**\n",
        "\n",
        "```python\n",
        "import graphviz\n",
        "from sklearn import tree\n",
        "\n",
        "tree_data = tree.export_graphviz(model_DT, out_file=None)\n",
        "graph = graphviz.Source(tree_data)\n",
        "graph\n",
        "```\n",
        "\n",
        "## **Validação cruzada Regressão**\n",
        "\n",
        "* MAE/RSME\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(model, x_test, y_test, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Converte o erro quadrático médio negativo para erro quadrático médio\n",
        "mse_scores = -scores\n",
        "# Calcula o RMSE\n",
        "rmse_scores = np.sqrt(mse_scores)\n",
        "\n",
        "# Imprime cada score de RMSE\n",
        "for score in rmse_scores:\n",
        "    print(\"RMSE score:\", score)\n",
        "\n",
        "print(\"\\nMédia:\", rmse_scores.mean())\n",
        "print(\"Desvio padrão:\", rmse_scores.std())\n",
        "```\n",
        "* R²\n",
        "\n",
        "```python\n",
        "# Métrica: R²\n",
        "r2 = cross_val_score(model, x_test, y_test, cv=5, scoring='r2')\n",
        "\n",
        "# Imprime cada score de R²:\n",
        "for score in r2:\n",
        "    print(\"R²:\", score)\n",
        "\n",
        "print(\"\\nMédia:\", r2.mean())\n",
        "print(\"Desvio padrão:\", r2.std())\n",
        "```\n",
        "\n",
        "* Scores: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-string-names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Tunning Hiper-parâmetros**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* GridSearch\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Modelo:\n",
        "random_forest = RandomForestRegressor()\n",
        "\n",
        "# Parâmetros\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, 30],\n",
        "    'bootstrap': [True, False] # amostragem com reposição/sem reposição\n",
        "}\n",
        "\n",
        "# Tunning com GridSearch:\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=random_forest,\n",
        "    param_grid=param_grid, cv=5,\n",
        "     scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Parâmetros para o modelo tunado:\n",
        "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation score: {-grid_search.best_score_}\")\n",
        "\n",
        "# Validação:\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(x_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = mse**0.5\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error on test set: {mse}\")\n",
        "print(f\"Root Mean Squared Error on test set: {rmse}\")\n",
        "print(f\"R² score on test set: {r2}\")\n",
        "``` \n",
        "\n",
        "* Optuna \n",
        "\n",
        "```python\n",
        "import optuna\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Carregar dados\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Divisão treino/validação\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "def objective(trial):\n",
        "    # Definir hiperparâmetros a serem otimizados\n",
        "    n_neighbors = trial.suggest_int(\"n_neighbors\", 1, 30)\n",
        "    weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n",
        "    p = trial.suggest_int(\"p\", 1, 3)\n",
        "\n",
        "    # Criar modelo KNN\n",
        "    model = KNeighborsRegressor(\n",
        "        n_neighbors=n_neighbors,\n",
        "        weights=weights,\n",
        "        p=p\n",
        "    )\n",
        "\n",
        "    # Treinar e validar\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_val)\n",
        "    mse = mean_squared_error(y_val, preds)\n",
        "    return mse  # Minimizar erro quadrático médio\n",
        "\n",
        "# Criar estudo e otimizar\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "print(\"Melhores hiperparâmetros:\", study.best_params)\n",
        "print(\"Melhor MSE:\", study.best_value)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtsz6rLVNfjm"
      },
      "source": [
        "## **Exportando um modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq0_MdJ2NdkH"
      },
      "source": [
        "\n",
        "```python\n",
        "import pickle\n",
        "\n",
        "with open('rf.pkl', 'wb') as file:\n",
        "  pickle.dump(rf, file)\n",
        "```\n",
        "\n",
        "```python\n",
        "with open('rf.pkl', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "loaded_model\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
